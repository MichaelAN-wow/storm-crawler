This has been generated by the StormCrawler Maven Archetype as a starting point for building your own crawler with Elasticsearch as a backend.
Have a look at the code and resources and modify them to your heart's content. 

First generate an uberjar:

``` sh
mvn clean package
```

then with Elasticsearch running locally, run `./ES_IndexInit.sh` to define the indices used by StormCrawler.

The first step consists in creating a file _seeds.txt_ in the current directory and populating it with the URLs 
to be used as a starting point for the crawl, e.g. 

`echo "http://stormcrawler.net/" > seeds.txt`

You can start the crawl topology using the Java class

``` sh
storm jar target/${artifactId}-${version}.jar ${package}.ESCrawlTopology -conf crawler-conf.yaml -conf es-conf.yaml -local . seeds.txt
```

This will run the topology in local mode, using the URLs in _seeds.txt_ as a starting point.

Simply remove the '-local' to run the topology in distributed mode with Apache Storm installed as a service.

Alternatively, you can also use Flux to do the same:

``` sh
storm jar target/${artifactId}-${version}.jar  org.apache.storm.flux.Flux --local es-crawler.flux --sleep 86400000
```

The command above runs the topology for 24 hours.


It is best to run the topology with `--remote` to benefit from the Storm UI and logging. In that case, the topology runs continuously, as intended.


Happy crawling! If you have any questions, please ask on [StackOverflow with the tag stormcrawler](http://stackoverflow.com/questions/tagged/stormcrawler). 



